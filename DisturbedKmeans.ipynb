{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"96816ed7-b08a-4ca3-abb9-f99880c3535d","showTitle":false,"title":""}},"source":["## Overview\n","\n","This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n","\n","This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyspark\n","  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n","Collecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py): started\n","  Building wheel for pyspark (setup.py): finished with status 'done'\n","  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764027 sha256=64845c8e1f4c668f42f64e937befddc78db5f7b130e5d8d2a8cddc3dac7ae80f\n","  Stored in directory: c:\\users\\amitf\\appdata\\local\\pip\\cache\\wheels\\05\\75\\73\\81f84d174299abca38dd6a06a5b98b08ae25fce50ab8986fa1\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install pyspark"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6482be4c-f067-47c9-b0ac-35c938b94601","showTitle":false,"title":""}},"outputs":[],"source":["# File location and type\n","# File location and type\n","file_location_iris = \"/FileStore/tables/iris.csv\"\n","file_location_glass = \"/FileStore/tables/glass.csv\"\n","file_location_parkisons = \"/FileStore/tables/parkinsons.csv\"\n","file_type = \"csv\"\n","\n","# CSV options\n","infer_schema = \"true\"\n","first_row_is_header = \"true\"\n","delimiter = \",\"\n","\n","# The applied options are for CSV files. For other file types, these will be ignored.\n","df = spark.read.format(file_type) \\\n","  .option(\"inferSchema\", infer_schema) \\\n","  .option(\"header\", first_row_is_header) \\\n","  .option(\"sep\", delimiter) \\\n","  .load(file_location_parkisons)\n","\n","display(df)"]},{"cell_type":"code","execution_count":5,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"db9631f6-bb4a-42ca-8a3c-0d48af932331","showTitle":false,"title":""}},"outputs":[],"source":["import numpy as np\n","from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n","from pyspark.ml.functions import vector_to_array\n","from pyspark.ml import Pipeline\n","import math\n","from sklearn.metrics.cluster import adjusted_rand_score\n","from sklearn.cluster import KMeans\n","from sklearn import metrics\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b0daf52b-be0d-4c40-9e37-b39a1acdb208","showTitle":false,"title":""}},"outputs":[],"source":["def scale_data(df):\n","    # convert to vector for calculations\n","    assembler = VectorAssembler(inputCols=df.columns[:-1], outputCol=\"features\")\n","    # scale with min max scale\n","    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n","    pipeline = Pipeline(stages=[assembler, scaler])\n","    scalerModel = pipeline.fit(df)\n","    scaledData = scalerModel.transform(df)\n","    points_scale = scaledData.select(\"scaledFeatures\")\n","    points_with_array = points_scale.withColumn(\"scaledFeatures\", vector_to_array(\"scaledFeatures\"))\n","    points_change = points_with_array.rdd.map(lambda p: p[0])\n","    return_points = points_change.map(lambda p: tuple(p))\n","    return return_points"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5b2fae11-5878-443f-a58f-fba469458955","showTitle":false,"title":""}},"outputs":[],"source":["def sum_points(pointsA,pointsB):\n","    final_list = list(np.array(pointsA) + np.array(pointsB))\n","    return final_list\n"," \n","\n","def distance(centroid, point):\n","    sub_list = list(np.array(centroid) - np.array(point))\n","    distance_vector_square = np.power(sub_list, 2)\n","    final = np.sqrt(np.sum(distance_vector_square))\n","    return final\n","        \n","def divide(all_g, c):\n","    return [val / c for val in all_g]\n"]},{"cell_type":"code","execution_count":6,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"63f1dfb9-f025-460d-b577-86a530b3c569","showTitle":false,"title":""}},"outputs":[],"source":["# allocate centroids\n","def assign_centroids(points,centroids):\n","    min_dist = math.inf\n","    nearest_centroid = 0\n","    for i in range(len(centroids)):\n","        distance_point = distance(centroids[i], points)\n","        if(distance_point < min_dist):\n","            min_dist = distance_point\n","            nearest_centroid = i\n","    return (nearest_centroid, points)\n","\n","\n","# function that checks that all the clusters are in the right the threshold \n","def threshold_check(last_c, new_c, threshold):\n","    avg_difference = []\n","    for i in range(len(new_c)):\n","        lst = []\n","        for j in range(len(new_c[0])):\n","            lst.append(np.sqrt(np.power(new_c[i][j] - last_c[i][j],2)))\n","        avg_difference.append(np.mean(lst))\n","    counter = 0\n","    for avg in avg_difference:\n","        if avg < threshold:\n","            counter += 1\n","    # means that all centers diffrence were minor (less than 0.0001)\n","    return True if counter == len(avg_difference) else False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1fdc0a0a-13f2-470b-a5a7-f0f43c95bdc6","showTitle":false,"title":""}},"outputs":[],"source":["def K_means(data,k,CT=0.0001,I=30):\n","    # take all scaled points from the data\n","    points = scale_data(data)\n","    # initial centroids\n","    centroids = points.takeSample(False, k)\n","    count=0\n","    while (count < I):\n","        # map raduce functions\n","        map_func = lambda val: (val[0], (val[1], 1))\n","        reduce_func = lambda val1, val2: ((sum_points(val1[0], val2[0]), val1[1] + val2[1]))\n","        points2 = points.map(lambda val: assign_centroids(val,centroids))\n","        last_center = centroids\n","        # do the map reduce using rdd \n","        summ  = points2.map(map_func)\n","        now_centroids = summ.reduceByKey(reduce_func).mapValues(lambda p: divide(p[0],p[1])).collect()\n","        # ectracr the centroids\n","        centroids = [p[1] for p in now_centroids]\n","        # if all pass the threshold- return the centers\n","        if threshold_check(last_center,centroids,CT):\n","            return now_centroids\n","        # else count = count +1 and continue to next iteration\n","        count += 1\n","    return now_centroids"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"387d1549-8664-41e5-876b-268154f95eba","showTitle":false,"title":""}},"outputs":[],"source":["# calculate the Calinski and harabasz score of the new centroids and the division of the points.\n","def get_ch(end_center_points, data):\n","    # get new class\n","    new_class = end_center_points.map(lambda d: d[0])\n","    points = end_center_points.map(lambda d: d[1])\n","    # evaluate the algorithem \n","    get_ch = metrics.calinski_harabasz_score(points.collect(), new_class.collect())\n","    return get_ch\n","\n","# Calculate the Adjusted rand index. Computes a similarity measure between two clusterings, the original and the new one.\n","def get_ari(end_center_points, data):\n","    # get original class \n","    classes = data.select(\"class\")\n","    class_original = classes.rdd.map(lambda d: d[0])\n","    # get new class\n","    new_class = end_center_points.map(lambda d: d[0])\n","    #calculate similarity\n","    get_ari = adjusted_rand_score(new_class.collect(), class_original.collect())         \n","    return get_ari"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9bead2b7-b0b9-44f4-94ad-f7ff1b1728df","showTitle":false,"title":""}},"outputs":[],"source":["def K_means_scores(datapath,k,CT=0.0001,I=30,Exp=10):\n","    # read the data\n","    data = spark.read.csv(datapath, header = True, inferSchema = True)\n","    CH,ARI = [], []\n","    centroids = K_means(data,k,CT,I)\n","    points = scale_data(data)\n","    for _ in range(Exp):\n","        centroids = K_means(data,k,CT,I)\n","        centroids_vals = [i[1] for i in centroids]\n","        points_rdd = points.map(lambda d: assign_centroids(d,centroids_vals))\n","        get_ch_check = get_ch(points_rdd, data)\n","        CH.append(get_ch_check)\n","        get_ari_check = get_ari(points_rdd, data)\n","        ARI.append(get_ari_check)\n","    mean_CH,mean_ARI = np.mean(CH),np.mean(ARI)\n","    std_CH, std_ARI = np.std(CH),np.std(ARI)\n","    CH_values = mean_CH, std_CH\n","    ARI_values = mean_ARI, std_ARI\n","    result = [CH_values, ARI_values]\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9c89e3ca-ddb9-4806-9454-3fc4f39e14fd","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["[(294.5619181149922, 15.516159235247006), (0.6027895424100469, 0.018428318966144205)]\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"[(294.5619181149922, 15.516159235247006), (0.6027895424100469, 0.018428318966144205)]\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["print(K_means_scores(file_location_iris,4))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"babd2cc6-dbc3-41db-af32-281e54669b1a","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["the scores for 3 clusters in iris:\n"," mean of chalinski is 320.67, std chalinski is 60.55\n"," mean of ari is 0.65, std ari is 0.11\n","the scores for 5 clusters in iris:\n"," mean of chalinski is 250.9, std chalinski is 25.28\n"," mean of ari is 0.56, std ari is 0.07\n","the scores for 7 clusters in iris:\n"," mean of chalinski is 223.41, std chalinski is 19.09\n"," mean of ari is 0.46, std ari is 0.07\n","the scores for 9 clusters in iris:\n"," mean of chalinski is 215.51, std chalinski is 17.51\n"," mean of ari is 0.41, std ari is 0.05\n","the scores for 11 clusters in iris:\n"," mean of chalinski is 186.89, std chalinski is 18.39\n"," mean of ari is 0.36, std ari is 0.06\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"the scores for 3 clusters in iris:\n mean of chalinski is 320.67, std chalinski is 60.55\n mean of ari is 0.65, std ari is 0.11\nthe scores for 5 clusters in iris:\n mean of chalinski is 250.9, std chalinski is 25.28\n mean of ari is 0.56, std ari is 0.07\nthe scores for 7 clusters in iris:\n mean of chalinski is 223.41, std chalinski is 19.09\n mean of ari is 0.46, std ari is 0.07\nthe scores for 9 clusters in iris:\n mean of chalinski is 215.51, std chalinski is 17.51\n mean of ari is 0.41, std ari is 0.05\nthe scores for 11 clusters in iris:\n mean of chalinski is 186.89, std chalinski is 18.39\n mean of ari is 0.36, std ari is 0.06\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["k_range_1, k_range_2, k_range_3 = [3,5,7,9,11],[2,4,6,8,10], [2,3,5,7,9]\n","\n","# file_location_iris\n","for k in k_range_1:\n","    result = K_means_scores(file_location_iris, k = k)\n","    print(f\"the scores for {k} clusters in iris:\")\n","    print(f\" mean of chalinski is {round(result[0][0],2)}, std chalinski is {round(result[0][1],2)}\")\n","    print(f\" mean of ari is {round(result[1][0],2)}, std ari is {round(result[1][1],2)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"eb526f26-ce99-44dc-b4d8-5e10dc1bcd66","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["the scores for 2 clusters in glass:\n"," mean of chalinski is 173.28, std chalinski is 0.24\n"," mean of ari is 0.21,  std  ari is 0.01\n","the scores for 4 clusters in glass:\n"," mean of chalinski is 93.83, std chalinski is 6.99\n"," mean of ari is 0.16,  std  ari is 0.06\n","the scores for 6 clusters in glass:\n"," mean of chalinski is 83.17, std chalinski is 9.11\n"," mean of ari is 0.18,  std  ari is 0.03\n","the scores for 8 clusters in glass:\n"," mean of chalinski is 75.06, std chalinski is 9.0\n"," mean of ari is 0.17,  std  ari is 0.03\n","the scores for 10 clusters in glass:\n"," mean of chalinski is 63.27, std chalinski is 7.53\n"," mean of ari is 0.19,  std  ari is 0.03\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"the scores for 2 clusters in glass:\n mean of chalinski is 173.28, std chalinski is 0.24\n mean of ari is 0.21,  std  ari is 0.01\nthe scores for 4 clusters in glass:\n mean of chalinski is 93.83, std chalinski is 6.99\n mean of ari is 0.16,  std  ari is 0.06\nthe scores for 6 clusters in glass:\n mean of chalinski is 83.17, std chalinski is 9.11\n mean of ari is 0.18,  std  ari is 0.03\nthe scores for 8 clusters in glass:\n mean of chalinski is 75.06, std chalinski is 9.0\n mean of ari is 0.17,  std  ari is 0.03\nthe scores for 10 clusters in glass:\n mean of chalinski is 63.27, std chalinski is 7.53\n mean of ari is 0.19,  std  ari is 0.03\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["# file_location_glass\n","for k in k_range_2:\n","    result = K_means_scores(file_location_glass, k = k)\n","    print(f\"the scores for {k} clusters in glass:\")\n","    print(f\" mean of chalinski is {round(result[0][0],2)}, std chalinski is {round(result[0][1],2)}\")\n","    print(f\" mean of ari is {round(result[1][0],2)},  std  ari is {round(result[1][1],2)}\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d4f019f2-1ce9-49b3-8581-bb63f11a39e4","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["the scores for 2 clusters in glass:\n"," mean of chalinski is 173.39, std ari is 0.23\n"," mean of ari is 0.2,  std ari is 0.01\n","the scores for 3 clusters in glass:\n"," mean of chalinski is 112.77, std ari is 8.88\n"," mean of ari is 0.2,  std ari is 0.06\n","the scores for 5 clusters in glass:\n"," mean of chalinski is 88.22, std ari is 9.74\n"," mean of ari is 0.19,  std ari is 0.03\n","the scores for 7 clusters in glass:\n"," mean of chalinski is 79.79, std ari is 12.01\n"," mean of ari is 0.17,  std ari is 0.02\n","the scores for 9 clusters in glass:\n"," mean of chalinski is 69.06, std ari is 9.89\n"," mean of ari is 0.19,  std ari is 0.02\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"the scores for 2 clusters in glass:\n mean of chalinski is 173.39, std ari is 0.23\n mean of ari is 0.2,  std ari is 0.01\nthe scores for 3 clusters in glass:\n mean of chalinski is 112.77, std ari is 8.88\n mean of ari is 0.2,  std ari is 0.06\nthe scores for 5 clusters in glass:\n mean of chalinski is 88.22, std ari is 9.74\n mean of ari is 0.19,  std ari is 0.03\nthe scores for 7 clusters in glass:\n mean of chalinski is 79.79, std ari is 12.01\n mean of ari is 0.17,  std ari is 0.02\nthe scores for 9 clusters in glass:\n mean of chalinski is 69.06, std ari is 9.89\n mean of ari is 0.19,  std ari is 0.02\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["# file_location_parkinsons\n","for k in k_range_3:\n","    result = K_means_scores(file_location_glass, k = k)\n","    print(f\"the scores for {k} clusters in glass:\")\n","    print(f\" mean of chalinski is {round(result[0][0],2)}, std ari is {round(result[0][1],2)}\")\n","    print(f\" mean of ari is {round(result[1][0],2)},  std ari is {round(result[1][1],2)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f21b037f-ba36-4f17-a619-80968bc93464","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"2022-06-15 - DBFS Example","notebookOrigID":792181075014943,"widgets":{}},"interpreter":{"hash":"c6ead00430738702e4382c90db75dceb92a0ebcb196c05fbe1d9ad27b21d47cf"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
